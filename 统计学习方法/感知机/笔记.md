# 感知机模型
## 1.定义：$ f(x) = sign( \omega x+b ) $ sign为符号函数，ω为权重，b为偏置
### $ sign(x)  = \begin{cases} 1,& x\ge0 \\ -1,& x\lt0 \end{cases}$
# 感知机学习策略
## 1.线性可分性：存在超平面$\omega x+b=0$将正负实例点分隔开，使得$y_i( \omega x_i+b) \gt0$，则称该数据集线性可分
## 2.感知机学习策略
### 学习目标：求得一个能将训练集中正负实例点完全分开的超平面
### 误分类的数据：$-y_i( \omega x_i+b+) \gt0$
### 误分类点到超平面距离：$- \frac 1{\vert\vert\omega\vert\vert}y_i( \omega x_i+b)$
### 所有误分类点到超平面的距离：$- \frac 1{\vert\vert\omega\vert\vert}\sum{x_i\in M}y_i( \omega x_i+b)$
### 不考虑系数得到感知机损失函数：$-\sum{x_i\in M}y_i( \omega x_i+b)$
# 感知机学习算法
## 1. 感知机学习算法的原始形式
### 随机梯度下降法来更新参数
### $\nabla_\omega L(\omega,b)=-\sum{x_i\in M}y_ix_i$
### $\nabla_b L(\omega,b)=-\sum{x_i\in M}y_i$
### 参数更新：$\omega\longleftarrow\omega+\eta y_ix_i \\                     b\longleftarrow b+\eta    y_i$
### 其中$\eta$为学习率，$\eta \in (0,1)$
### 出现$y_i( \omega x_i+b)\leq 0$则对参数进行更新
## 2. 算法的收敛性
### 记作 $\hat \omega \cdot\hat x = \omega x+b$
### 2.1 存在满足条件$\vert\vert \hat \omega_{opt} \vert\vert=1$的超平面$\hat \omega_{opt} \cdot \hat x=\omega_{opt} \cdot x+b_{opt}=0$，存在$\gamma\gt0$使得$y_i(\hat \omega_{opt} \cdot \hat x_i)=y_i(\omega_{opt} \cdot x_i+b_{opt})\gt0$ 
### 2.2 令$R=\max_{1\leqslant i \leqslant N}\vert\vert \hat x_i \vert\vert$，再训练集上误分类次数k满足$k\leqslant (\frac{R}{\gamma})^2$
## 3. 感知机学习算法的对偶形式
### N次迭代后的参数表述
### $\omega = \sum_{i=1}^N \alpha_i y_i x_i$
### $b =\sum_{i=1}^N \alpha_i y_i$
### 对偶形式
#### 用Gram矩阵来存储实例间的内积  [参考链接](https://blog.csdn.net/wangyang20170901/article/details/79037867)
#### 1、$\omega\longleftarrow0$
#### $ b\longleftarrow0$
#### 2、存在误分类的点$y_i(\sum_{j=1}^N\alpha_j y_j x_j \cdot x_i+b)\leqslant0$则进行参数更新
#### $\omega_i \longleftarrow \alpha_i + \eta $
#### $b \longleftarrow b+\eta y_i$
#### 重复(2)直到无误分类数据